{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939041b9",
   "metadata": {},
   "source": [
    "# Time generating predictions for first 20 observations of test set\n",
    "\n",
    "To compare inference time to the other two models, time the predictions for the first 20 observations of the test set.\n",
    "\n",
    "## NowcastPNN with DOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9e5ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silaskoemen/Library/CloudStorage/OneDrive-ImperialCollegeLondon/Imperial/NowcastPNN/src/data_functions.py:108: DtypeWarning: Columns (7,11,23,45,46,47,55,65,69,75,86,102) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dengdf = pd.read_csv(path, index_col=0)#pd.read_csv(f\"../data/derived/DENG{state}.csv\", index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2922 2013-01-01 00:00:00 2020-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "from data_functions import get_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler as SRS\n",
    "from train_utils import SubsetSampler as SS\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "import torch, random, os, numpy as np\n",
    "torch.use_deterministic_algorithms(True) # reproducibility\n",
    "\n",
    "STATE = \"SP\"\n",
    "WEEKS = False\n",
    "TRIANGLE = True\n",
    "PAST_UNITS = 40\n",
    "MAX_DELAY = 40\n",
    "BATCH_SIZE = 64\n",
    "RANDOM_SPLIT = True\n",
    "SEED = 1234\n",
    "DEVICE = \"mps\"\n",
    "DOW = True\n",
    "\n",
    "dataset_dow = get_dataset(weeks=WEEKS, triangle=TRIANGLE, past_units=PAST_UNITS, max_delay=MAX_DELAY, state=STATE, dow = DOW)\n",
    "#n_obs_40pu = len(dataset) # 2922 total dates, -39-39 for past_units and max_delay ->2844\n",
    "## Define train and test indices\n",
    "if RANDOM_SPLIT:\n",
    "    all_idcs = range(dataset_dow.__len__())\n",
    "    train_idcs, test_idcs = TTS(all_idcs, test_size=0.25, shuffle=True, random_state=SEED)\n",
    "    train_idcs, val_idcs = TTS(train_idcs, test_size=0.25, shuffle=True, random_state=SEED)\n",
    "    #train_idcs, test_idcs = [*range(600), *range(950, dataset.__len__())], [*range(600, 950)]\n",
    "    VAL_BATCH_SIZE, TEST_BATCH_SIZE = len(val_idcs), len(test_idcs)\n",
    "else:\n",
    "    if WEEKS:\n",
    "        train_idcs, test_idcs = range(300), range(300, dataset_dow.__len__())\n",
    "        TEST_BATCH_SIZE = dataset_dow.__len__() - 300\n",
    "    else: \n",
    "        train_idcs, test_idcs = range(int(0.75*dataset_dow.__len__())), range(int(0.75*dataset_dow.__len__()), dataset_dow.__len__()) # 2844 total obs - 711 test, still 25% even without random split dataset.__len__(), 2353\n",
    "        train_idcs, val_idcs = TTS(train_idcs, test_size=0.25, shuffle=True, random_state=SEED)\n",
    "        VAL_BATCH_SIZE, TEST_BATCH_SIZE = len(val_idcs), len(test_idcs)\n",
    "        \n",
    "## Define generator so sampling during training is deterministic and reproducible\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "train_sampler_dow, val_sampler_dow, test_sampler_dow = SRS(train_idcs, generator=g), SRS(val_idcs), SS(test_idcs)\n",
    "train_loader_dow, val_loader_dow, test_loader_dow = DataLoader(dataset_dow, batch_size=BATCH_SIZE, sampler=train_sampler_dow), DataLoader(dataset_dow, batch_size=VAL_BATCH_SIZE, sampler=val_sampler_dow, shuffle=False), DataLoader(dataset_dow, batch_size=TEST_BATCH_SIZE, sampler=test_sampler_dow, shuffle=False)\n",
    "\n",
    "## Function to reset the sampler so each training run uses same order of observations for reproducibility\n",
    "## Possible to define s.t. returns train_loader, but bc in notebook, possible to define globally\n",
    "def regen_data():\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    global train_loader_dow\n",
    "    train_loader_dow = DataLoader(dataset_dow, batch_size=BATCH_SIZE, sampler=SRS(train_idcs, generator=g))\n",
    "\n",
    "def regen_loader(seed, dataset, batch_size, idcs):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    gen_loader = DataLoader(dataset, batch_size=batch_size, sampler = SRS(idcs, generator = g))\n",
    "    return gen_loader\n",
    "\n",
    "def set_seeds(SEED):\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seeds(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47fdcb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train_utils import train, EarlyStopper\n",
    "from NowcastPNN import NowcastPNNDOW\n",
    "set_seeds(SEED) # reproducible training runs\n",
    "regen_data()\n",
    "nowcast_pnn_dow = NowcastPNNDOW(past_units=PAST_UNITS, max_delay=MAX_DELAY, dropout_probs=[0.3, 0.1] if RANDOM_SPLIT else [0.15, 0.1]) # 0.3, 0.1 best for random split, 0.15, 0.1 for recent\n",
    "nowcast_pnn_dow.load_state_dict(torch.load(f\"./weights/weights-{PAST_UNITS}-{MAX_DELAY}-{'week' if WEEKS else 'day'}-fut0{'-rec' if not RANDOM_SPLIT else ''}{'-dow' if DOW else ''}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80afc72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0041, 0.0196, 0.0200,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0065, 0.0227, 0.0193,  ..., 0.0003, 0.0000, 0.0000],\n",
      "        [0.0100, 0.0272, 0.0096,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0592, 0.0926, 0.0320,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0503, 0.0424, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0368, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='mps:0') torch.Size([40, 40])\n",
      "tensor(2, device='mps:0') torch.Size([])\n",
      "tensor([1253.], device='mps:0') torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "test_obs = test_loader_dow.dataset[0]\n",
    "for o in test_obs:\n",
    "    if isinstance(o, tuple):\n",
    "        for ob in o:\n",
    "            print(ob, ob.shape)\n",
    "    else:\n",
    "        print(o, o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bbee8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through first 20 obs of test set, create list of times to compare\n",
    "from time import time\n",
    "\n",
    "NUM_TIMING_OBS = 20\n",
    "N_SAMPLES = 200\n",
    "inference_times = []\n",
    "\n",
    "for i in range(NUM_TIMING_OBS):\n",
    "    mat, y = test_loader_dow.dataset.__getitem__(test_loader_dow.sampler.indices[i])\n",
    "    mat, dow_val = mat\n",
    "    dow_val = torch.unsqueeze(dow_val.to(\"cpu\"), 0)\n",
    "    mat, y = torch.unsqueeze(mat.to(\"cpu\"), 0), np.expand_dims(y.to(\"cpu\").numpy(), 0)\n",
    "    nowcast_pnn_dow.eval() # sets batch norm to eval so a single entry can be passed without issues of calculating mean and std and overall means and stds used\n",
    "    nowcast_pnn_dow.drop1.train() # keeps dropout layers active\n",
    "    nowcast_pnn_dow.drop2.train()\n",
    "    nowcast_pnn_dow = nowcast_pnn_dow.to(\"cpu\")\n",
    "    start = time()\n",
    "    preds = np.zeros((y.shape[0], N_SAMPLES))\n",
    "    for i in range(N_SAMPLES):\n",
    "        #preds[:, i] = np.squeeze(nowcast_pnn_dow(mat).sample().numpy())\n",
    "        preds[:, i] = nowcast_pnn_dow(mat, dow_val).sample().numpy()\n",
    "    min_preds, max_preds = np.min(preds, axis=1), np.max(preds, axis=1)\n",
    "    pred_median = np.quantile(preds, 0.5, axis=1)\n",
    "    inference_times.append(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b3f836f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1818275570869446, 0.014438801530890822)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(inference_times), np.std(inference_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da36904a",
   "metadata": {},
   "source": [
    "### Time prediction of entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2d3c1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.865705966949463\n"
     ]
    }
   ],
   "source": [
    "(mat, dow), y = next(iter(test_loader_dow))\n",
    "mat, dow = mat.to(\"cpu\"), dow.to(\"cpu\")\n",
    "y = y.to(\"cpu\").numpy()\n",
    "nowcast_pnn_dow.eval() # sets batch norm to eval so a single entry can be passed without issues of calculating mean and std and overall means and stds used\n",
    "nowcast_pnn_dow.drop1.train() # keeps dropout layers active\n",
    "nowcast_pnn_dow.drop2.train()\n",
    "nowcast_pnn_dow = nowcast_pnn_dow.to(\"cpu\")\n",
    "start_time = time()\n",
    "preds = np.zeros((y.shape[0], N_SAMPLES))\n",
    "for i in range(N_SAMPLES):\n",
    "    preds[:, i] = nowcast_pnn_dow(mat, dow_val).sample().numpy()\n",
    "min_preds, max_preds = np.min(preds, axis=1), np.max(preds, axis=1)\n",
    "pred_median = np.quantile(preds, 0.5, axis=1)\n",
    "print(time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6671506",
   "metadata": {},
   "source": [
    "## NowcastPNN without DOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1afdef2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/silaskoemen/Library/CloudStorage/OneDrive-ImperialCollegeLondon/Imperial/NowcastPNN/src/data_functions.py:108: DtypeWarning: Columns (7,11,23,45,46,47,55,65,69,75,86,102) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dengdf = pd.read_csv(path, index_col=0)#pd.read_csv(f\"../data/derived/DENG{state}.csv\", index_col=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2922 2013-01-01 00:00:00 2020-12-31 00:00:00\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "from data_functions import get_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler as SRS\n",
    "from train_utils import SubsetSampler as SS\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "import torch, random, os, numpy as np\n",
    "torch.use_deterministic_algorithms(True) # reproducibility\n",
    "\n",
    "STATE = \"SP\"\n",
    "WEEKS = False\n",
    "TRIANGLE = True\n",
    "PAST_UNITS = 40\n",
    "MAX_DELAY = 40\n",
    "BATCH_SIZE = 64\n",
    "RANDOM_SPLIT = True\n",
    "SEED = 1234\n",
    "DEVICE = \"mps\"\n",
    "DOW = False\n",
    "\n",
    "dataset = get_dataset(weeks=WEEKS, triangle=TRIANGLE, past_units=PAST_UNITS, max_delay=MAX_DELAY, state=STATE, dow = DOW)\n",
    "#n_obs_40pu = len(dataset) # 2922 total dates, -39-39 for past_units and max_delay ->2844\n",
    "## Define train and test indices\n",
    "if RANDOM_SPLIT:\n",
    "    all_idcs = range(dataset.__len__())\n",
    "    train_idcs, test_idcs = TTS(all_idcs, test_size=0.25, shuffle=True, random_state=SEED)\n",
    "    train_idcs, val_idcs = TTS(train_idcs, test_size=0.25, shuffle=True, random_state=SEED)\n",
    "    #train_idcs, test_idcs = [*range(600), *range(950, dataset.__len__())], [*range(600, 950)]\n",
    "    VAL_BATCH_SIZE, TEST_BATCH_SIZE = len(val_idcs), len(test_idcs)\n",
    "else:\n",
    "    if WEEKS:\n",
    "        train_idcs, test_idcs = range(300), range(300, dataset.__len__())\n",
    "        TEST_BATCH_SIZE = dataset.__len__() - 300\n",
    "    else: \n",
    "        train_idcs, test_idcs = range(int(0.75*dataset.__len__())), range(int(0.75*dataset.__len__()), dataset.__len__()) # 2844 total obs - 711 test, still 25% even without random split dataset.__len__(), 2353\n",
    "        train_idcs, val_idcs = TTS(train_idcs, test_size=0.25, shuffle=True, random_state=SEED)\n",
    "        VAL_BATCH_SIZE, TEST_BATCH_SIZE = len(val_idcs), len(test_idcs)\n",
    "        \n",
    "## Define generator so sampling during training is deterministic and reproducible\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "train_sampler, val_sampler, test_sampler = SRS(train_idcs, generator=g), SRS(val_idcs), SS(test_idcs)\n",
    "train_loader, val_loader, test_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler), DataLoader(dataset, batch_size=VAL_BATCH_SIZE, sampler=val_sampler, shuffle=False), DataLoader(dataset, batch_size=TEST_BATCH_SIZE, sampler=test_sampler, shuffle=False)\n",
    "\n",
    "## Function to reset the sampler so each training run uses same order of observations for reproducibility\n",
    "## Possible to define s.t. returns train_loader, but bc in notebook, possible to define globally\n",
    "def regen_data():\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    global train_loader\n",
    "    train_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=SRS(train_idcs, generator=g))\n",
    "\n",
    "def regen_loader(seed, dataset, batch_size, idcs):\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    gen_loader = DataLoader(dataset, batch_size=batch_size, sampler = SRS(idcs, generator = g))\n",
    "    return gen_loader\n",
    "\n",
    "def set_seeds(SEED):\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seeds(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3867abd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from train_utils import train, EarlyStopper\n",
    "from NowcastPNN import NowcastPNN\n",
    "set_seeds(SEED) # reproducible training runs\n",
    "regen_data()\n",
    "nowcast_pnn = NowcastPNN(past_units=PAST_UNITS, max_delay=MAX_DELAY, dropout_probs=[0.3, 0.1] if RANDOM_SPLIT else [0.15, 0.1]) # 0.3, 0.1 best for random split, 0.15, 0.1 for recent\n",
    "nowcast_pnn.load_state_dict(torch.load(f\"./weights/weights-{PAST_UNITS}-{MAX_DELAY}-{'week' if WEEKS else 'day'}-fut0{'-rec' if not RANDOM_SPLIT else ''}{'-dow' if DOW else ''}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f8084e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through first 20 obs of test set, create list of times to compare\n",
    "from time import time\n",
    "\n",
    "NUM_TIMING_OBS = 20\n",
    "N_SAMPLES = 200\n",
    "inference_times = []\n",
    "\n",
    "for i in range(NUM_TIMING_OBS):\n",
    "    mat, y = test_loader.dataset.__getitem__(test_loader.sampler.indices[i])\n",
    "    mat, y = torch.unsqueeze(mat.to(\"cpu\"), 0), np.expand_dims(y.to(\"cpu\").numpy(), 0)\n",
    "    nowcast_pnn.eval() # sets batch norm to eval so a single entry can be passed without issues of calculating mean and std and overall means and stds used\n",
    "    nowcast_pnn.drop1.train() # keeps dropout layers active\n",
    "    nowcast_pnn.drop2.train()\n",
    "    nowcast_pnn = nowcast_pnn.to(\"cpu\")\n",
    "    start = time()\n",
    "    preds = np.zeros((y.shape[0], N_SAMPLES))\n",
    "    for i in range(N_SAMPLES):\n",
    "        #preds[:, i] = np.squeeze(nowcast_pnn(mat).sample().numpy())\n",
    "        preds[:, i] = nowcast_pnn(mat).sample().numpy()\n",
    "    min_preds, max_preds = np.min(preds, axis=1), np.max(preds, axis=1)\n",
    "    pred_median = np.quantile(preds, 0.5, axis=1)\n",
    "    inference_times.append(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e561c76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1423903226852417, 0.009843222814249606)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(inference_times), np.std(inference_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fd2f90",
   "metadata": {},
   "source": [
    "## Time prediction of entire test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd205b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.819590330123901\n"
     ]
    }
   ],
   "source": [
    "mat, y = next(iter(test_loader))\n",
    "mat = mat.to(\"cpu\")\n",
    "y = y.to(\"cpu\").numpy()\n",
    "nowcast_pnn.eval() # sets batch norm to eval so a single entry can be passed without issues of calculating mean and std and overall means and stds used\n",
    "nowcast_pnn.drop1.train() # keeps dropout layers active\n",
    "nowcast_pnn.drop2.train()\n",
    "nowcast_pnn = nowcast_pnn.to(\"cpu\")\n",
    "start_time = time()\n",
    "preds = np.zeros((y.shape[0], N_SAMPLES))\n",
    "for i in range(N_SAMPLES):\n",
    "    #preds[:, i] = np.squeeze(nowcast_pnn(mat).sample().numpy())\n",
    "    preds[:, i] = nowcast_pnn(mat).sample().numpy()\n",
    "min_preds, max_preds = np.min(preds, axis=1), np.max(preds, axis=1)\n",
    "pred_median = np.quantile(preds, 0.5, axis=1)\n",
    "print(time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b300d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
