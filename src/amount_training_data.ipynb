{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigation of how many observations in test set needed for good results\n",
    "\n",
    "Still uses the same test sets, so loss values are comparable across models and with benchmark models\n",
    "\n",
    "## First with most recent observations as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "from data_functions import get_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler as SRS\n",
    "from train_utils import SubsetSampler as SS\n",
    "from sklearn.model_selection import train_test_split as TTS\n",
    "import torch, random, os, numpy as np\n",
    "torch.use_deterministic_algorithms(True) # reproducibility\n",
    "\n",
    "STATE = \"SP\"\n",
    "WEEKS = False\n",
    "TRIANGLE = True\n",
    "PAST_UNITS = 40\n",
    "MAX_DELAY = 40\n",
    "BATCH_SIZE = 64\n",
    "RANDOM_SPLIT = False\n",
    "SEED = 1234\n",
    "DEVICE = \"mps\"\n",
    "DOW = True\n",
    "\n",
    "dataset= get_dataset(weeks=WEEKS, triangle=TRIANGLE, past_units=PAST_UNITS, max_delay=MAX_DELAY, state=STATE, dow = DOW)\n",
    "#n_obs_40pu = len(dataset) # 2922 total dates, -39-39 for past_units and max_delay ->2844\n",
    "## Define train and test indices\n",
    "if RANDOM_SPLIT:\n",
    "    all_idcs = range(dataset_dow.__len__())\n",
    "    train_idcs, test_idcs = TTS(all_idcs, test_size=0.25, shuffle=True, random_state=SEED)\n",
    "    train_idcs, val_idcs = TTS(train_idcs, test_size=0.25, shuffle=True, random_state=SEED)\n",
    "    #train_idcs, test_idcs = [*range(600), *range(950, dataset.__len__())], [*range(600, 950)]\n",
    "    VAL_BATCH_SIZE, TEST_BATCH_SIZE = len(val_idcs), len(test_idcs)\n",
    "else:\n",
    "    if WEEKS:\n",
    "        train_idcs, test_idcs = range(300), range(300, dataset.__len__())\n",
    "        TEST_BATCH_SIZE = dataset.__len__() - 300\n",
    "    else: \n",
    "        train_idcs, test_idcs = range(2133), range(2133, dataset.__len__()) # 2844 total obs - 711 test, still 25% even without random split dataset.__len__(), 2353\n",
    "        train_idcs, val_idcs = TTS(train_idcs, test_size=0.25, shuffle=True, random_state=SEED)\n",
    "        VAL_BATCH_SIZE, TEST_BATCH_SIZE = len(val_idcs), len(test_idcs)\n",
    "        \n",
    "## Define generator so sampling during training is deterministic and reproducible\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "train_sampler, val_sampler, test_sampler = SRS(train_idcs, generator=g), SRS(val_idcs), SS(test_idcs)\n",
    "train_loader, val_loader, test_loader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler), DataLoader(dataset, batch_size=VAL_BATCH_SIZE, sampler=val_sampler, shuffle=False), DataLoader(dataset, batch_size=TEST_BATCH_SIZE, sampler=test_sampler_dow, shuffle=False)\n",
    "\n",
    "## Function to reset the sampler so each training run uses same order of observations for reproducibility\n",
    "## Possible to define s.t. returns train_loader, but bc in notebook, possible to define globally\n",
    "def regen_data():\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    global train_loader_dow\n",
    "    train_loader_dow = DataLoader(dataset_dow, batch_size=BATCH_SIZE, sampler=SRS(train_idcs, generator=g))\n",
    "\n",
    "def set_seeds(SEED):\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seeds(SEED)\n",
    "\n",
    "def regen_data():\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(SEED)\n",
    "    global train_loaders\n",
    "    for i in range(FUTURE_OBS+1):\n",
    "        train_loaders[i] = DataLoader(dataset_collection[i], batch_size=BATCH_SIZE, sampler=SRS(train_idcs, generator=g))\n",
    "        \n",
    "train_loaders, val_loaders, test_loaders = [], [], []\n",
    "for i in range(FUTURE_OBS+1):\n",
    "    train_loaders.append(DataLoader(dataset_collection[i], batch_size=BATCH_SIZE, sampler=SRS(train_idcs, generator=g)))\n",
    "    val_loaders.append(DataLoader(dataset_collection[i], batch_size=len(val_idcs), sampler=SRS(val_idcs)))\n",
    "    test_loaders.append(DataLoader(dataset_collection[i], batch_size=len(test_idcs), sampler=SRS(test_idcs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the architecture is final, could just import from NowcastPNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from NegativeBinomial import NegBin as NB\n",
    "set_seeds(SEED)\n",
    "## For matrix-like (two-dimensional) input data\n",
    "class NowcastPNNDOW(nn.Module):\n",
    "    \"\"\" Still NowcastPNN, just this time processing the day of the week additionally to reporting triangle \"\"\"\n",
    "    def __init__(self, past_units = 40, max_delay = 40, hidden_units = [16, 8], conv_channels = [16, 1], embedding_dim = 10, load_embed = True):\n",
    "        super().__init__()\n",
    "        self.past_units = past_units\n",
    "        self.max_delay = max_delay\n",
    "        self.final_dim = past_units\n",
    "        self.conv1 = nn.Conv1d(self.max_delay, conv_channels[0], kernel_size=7, padding=\"same\")\n",
    "        self.conv2 = nn.Conv1d(conv_channels[0], conv_channels[1], kernel_size=7, padding=\"same\")\n",
    "        self.fc1 = nn.Linear(self.past_units, self.past_units)#, nn.Linear(self.past_units, self.past_units)\n",
    "        self.fc3, self.fc4 = nn.Linear(self.final_dim, hidden_units[0]), nn.Linear(hidden_units[0], hidden_units[1])#, nn.Linear(hidden_units[1], hidden_units[2])\n",
    "        #self.fc5 = nn.Linear(hidden_units[1], hidden_units[2])\n",
    "        self.fcnb = nn.Linear(hidden_units[-1], 2)\n",
    "        self.const = 10000 # if not normalized, take constant out\n",
    "        self.embedding_dim = embedding_dim\n",
    "        if load_embed:\n",
    "            self.embed = nn.Embedding.from_pretrained(torch.load(f\"./weights/embedding_weights_{embedding_dim}\").detach())\n",
    "        else:\n",
    "            self.embed = nn.Embedding(7, embedding_dim)\n",
    "        #self.embed.weight.requires_grad_(False)\n",
    "        #self.embed.weight = nn.Parameter(torch.randn((7, embedding_dim))), can use to initialize, doesn't help\n",
    "        self.fc_embed1, self.fc_embed2 = nn.Linear(embedding_dim, 2*embedding_dim), nn.Linear(2*embedding_dim, past_units)\n",
    "        #self.fc_embed1 = nn.Linear(embedding_dim, past_units)\n",
    "\n",
    "        self.bnorm1, self.bnorm2 = nn.BatchNorm1d(num_features=self.max_delay), nn.BatchNorm1d(num_features=conv_channels[0])#, nn.BatchNorm1d(num_features=conv_channels[1])#, nn.BatchNorm1d(num_features=conv_channels[2])\n",
    "        #self.bnorm3 = nn.BatchNorm1d(num_features=conv_channels[1])\n",
    "        self.bnorm5, self.bnorm6  = nn.BatchNorm1d(num_features=self.final_dim), nn.BatchNorm1d(num_features=hidden_units[0])#, nn.BatchNorm1d(num_features=hidden_units[2])\n",
    "        self.bnorm_embed = nn.BatchNorm1d(num_features=2*embedding_dim)\n",
    "        #self.bnorm7 = nn.BatchNorm1d(num_features=hidden_units[1])\n",
    "        self.bnorm_final = nn.BatchNorm1d(num_features=hidden_units[-1]) #hidden_units[1]/self.past_units for single model\n",
    "        self.attn1 = nn.MultiheadAttention(embed_dim=self.max_delay, num_heads=1, batch_first=True)\n",
    "        self.drop1, self.drop2 = nn.Dropout(0.1), nn.Dropout(0.1) \n",
    "        self.softplus = nn.Softplus()\n",
    "        self.act = nn.SiLU()\n",
    "    \n",
    "    def save_embeddings(self):\n",
    "        \"\"\" Allows the user to save the embeddings if trained with a different dimension\n",
    "        to load later and allow for reproducible training runs. Usage: run model with load_embed = False,\n",
    "        then use model.save_embeddings() after training and use the model with load_embed = True afterwards.\n",
    "        \"\"\"\n",
    "        torch.save(self.embed.weight, f\"./weights/embedding_weights_{self.embedding_dim}\")\n",
    "    \n",
    "    def forward(self, rep_tri, dow): ## Feed forward function, takes input of shape [batch, past_units, max_delay]\n",
    "        #x = x.permute(0, 2, 1) # [batch, past_units, max_delay] -> [batch, max_delay, past_units]\n",
    "        x = rep_tri.float()\n",
    "\n",
    "        ## Attention Block ##\n",
    "        x_add = x.clone()\n",
    "        x = self.attn1(x, x, x, need_weights = False)[0]\n",
    "        x = self.act(self.fc1(x.permute(0,2,1)))\n",
    "        x = x.permute(0,2,1) + x_add\n",
    "\n",
    "        ## Convolutional Block ##\n",
    "        x = x.permute(0, 2, 1) # [batch, past_units, max_delay] -> [batch, max_delay, past_units]\n",
    "        x = self.act(self.conv1(self.bnorm1(x)))\n",
    "        x = self.act(self.conv2(self.bnorm2(x)))\n",
    "        #x = self.act(self.conv3(self.bnorm3(x)))\n",
    "        #x = self.act(self.conv4(self.bnorm4(x)))\n",
    "        x = torch.squeeze(x)\n",
    "        #print(x)\n",
    "        ## Addition of embedding of day of the week ##\n",
    "        embedded = self.embed(dow)\n",
    "        #print(embedded)\n",
    "        x = x + self.act(self.fc_embed2(self.bnorm_embed(self.act(self.fc_embed1(embedded))))) # self.bnorm_embed1(embedded)\n",
    "        ## Fully Connected Block ##\n",
    "        x = self.drop1(x)\n",
    "        x = self.act(self.fc3(self.bnorm5(x)))\n",
    "        x = self.drop2(x)\n",
    "        x = self.act(self.fc4(self.bnorm6(x)))\n",
    "        #x = self.drop3(x)\n",
    "        #x = self.act(self.fc5(self.bnorm7(x)))\n",
    "        x = self.fcnb(self.bnorm_final(x))\n",
    "        dist = NB(lbda = self.const*self.softplus(x[:, 0]), phi = (self.const**2)*self.softplus(x[:, 1])+1e-5)\n",
    "        \"\"\"\n",
    "        x = self.fcpoi(self.bnorm_final(x))\n",
    "        dist = Poi(rate=self.const*self.softplus(x)+1e-5)\n",
    "        \"\"\"\n",
    "        return torch.distributions.Independent(dist, reinterpreted_batch_ndims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 3\n",
    "from NowcastPNN import NowcastPNNDOW\n",
    "from train_utils import train, EarlyStopper\n",
    "\n",
    "set_seeds(SEED)\n",
    "n_training = [500, 1000, 1500, 2000] # originally 2133 as training + val\n",
    "for n in n_training:\n",
    "    print(f\"##################### Number training data points: {n} #####################\")\n",
    "    ## Redefine training and validation indices, and define train and validation loaders\n",
    "    regen_data() # reset samplers so each training run is reproducible\n",
    "    early_stopper = EarlyStopper(patience=30, past_units=PAST_UNITS, max_delay=MAX_DELAY, weeks=WEEKS, random_split=RANDOM_SPLIT, dow = DOW, n_training = n)\n",
    "    nowcast_pnn = NowcastPNNDOW(past_units=PAST_UNITS, max_delay=MAX_DELAY, embedding_dim=10)\n",
    "    train(nowcast_pnn, num_epochs=200, train_loader=train_loaders[i], val_loader=val_loaders[i], early_stopper=early_stopper, loss_fct=\"nll\", device = DEVICE, dow = DOW)\n",
    "    ## Load best set of weights on test/validation set\n",
    "    #nowcast_pnn.load_state_dict(torch.load(f\"./weights/weights-{PAST_UNITS}-{MAX_DELAY}-{'week' if WEEKS else 'day'}-fut0{'-rec' if not RANDOM_SPLIT else ''}{'-dow' if DOW else ''}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import evaluate_PIs, pnn_PIs\n",
    "set_seeds(SEED) # Reproducible results\n",
    "\n",
    "for i in range(FUTURE_OBS):\n",
    "    print(f\"##################### Number training data points: {n} #####################\")\n",
    "    nowcast_pnn.load_state_dict(torch.load(f\"./weights/weights-{PAST_UNITS}-{MAX_DELAY}-{'week' if WEEKS else 'day'}-fut0{'-rec' if not RANDOM_SPLIT else ''}{'-dow' if DOW else ''}-{n}\"))\n",
    "    levels_pnn = pnn_PIs(nowcast_pnn, test_loaders[i], random_split = RANDOM_SPLIT, save=False, dow = DOW) # think about how save with name\n",
    "    _ = evaluate_PIs(levels_pnn, test_loaders[i])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
